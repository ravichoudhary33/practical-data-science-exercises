{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (15-688 only)\n",
    "In this problem, you're going to write a (very minimal) graph library, which uses both the adjacency dictionary and the sparse adjacency matrix representation of a graph.  Using these two representations, you'll implement two of the more well-known large-scale graph processing algorithms: Djisktra's algorithm for finding single-source shortest paths in the graph, and the PageRank algorithm for determining the importance of nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The test graph\n",
    "\n",
    "The graph we'll be focusing on for most of this assignment (and which we'll also be grading you on, we'll just be using a subset of the nodes that you won't know in advance), is a directed graph that represents the page links in the English language Wikipedia.  Specifically, we took the (pre-processed) Wikipedia dump from here: http://haselgrove.id.au/wikipedia.htm , which were taken from a 2008 version of Wikipedia, and we then selected only subselected only those nodes that had at least _500 incoming links_.  This resulted in a graph with about 24 thousands that had about 6 million edges between the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0: Warmup using networkx\n",
    "\n",
    "In this first part of the question: which will _not_ be graded (in the submitted version, you can't include the `networkx` library), you'll load the graph using `networkx` and compute shortest distances from a source node.  There are two files included with this notebook that are relevant here:\n",
    "\n",
    "    wikipedia_small.graph\n",
    "    wikipedia_small.nodes\n",
    "    \n",
    "The `.graph` file contains a list of integers, two per each line.  If the line \"`i j`\" appears in the file, this indicates a directed edge from node `i` to node `j`.  The `.nodes` file then contains a list of each node the the graph, where the link number indicates the node index.  This is how we can relate the node numbers in the `.graph` file to actual pages on Wikipedia.\n",
    "\n",
    "For Q0, load this graph into a networkx `DiGraph` object.  Note that the nodes in a networkx graph can be any hashable type (ints, strings, etc), so you can use any of these as a potential for the nodes (you could load loads corresponding to the integer indices from the `.graph` file, string versions of indices, or the node names themselves.  However you do it, though, you'll want to make sure that you can determine the actual Wikipedia page from the node number, and vice versa. (For what it's worth, we used the integer indices as node keys, and then created a separate dictionary that could look up the index from a node name).\n",
    "\n",
    "After you have loaded the graph, use the `nx.shortest_path_length()` function to get the link distance from the `Carnegie_Mellon_University` node to all other nodes in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "with open(\"wikipedia_small.graph\", \"r\") as f:\n",
    "    wiki_edgelst = [line.split(\" \") for line in f.read().split(\"\\n\")]\n",
    "with open(\"wikipedia_small.nodes\", \"r\") as f:\n",
    "    nodelst = [line for line in f.read().split(\"\\n\")]\n",
    "nodedict = dict(enumerate(nodelst))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this call, we get a maximum hop count of 5 to any other node in the network (or infinity, some nodes cannot be reached by our node, usually ones that have only outlinks but no inlinks from other nodes in the network, but networkx does not return these in the `shortest_path_length()` result).  One such node is the `List_of_Salticidae_species` page, which gets connected via the path (which you can find via `nx.shortest_path()`:\n",
    "\n",
    "    Carnegie_Mellon_University\n",
    "    California\n",
    "    Gasoline\n",
    "    Blood\n",
    "    Jumping_spider\n",
    "    List_of_Salticidae_species\n",
    "\n",
    "(note that not all these links appear to still occur in the current version of Wikipedia).  There are also calls in newtorkx that retrieve and adjacency matrix and run the PageRank algorithm, but these are quite slow, and would take too long to run on a graph this size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your own Graph class\n",
    "\n",
    "In the main portion of this assignment, you'll create your own Graph class that mimics some of the functionality of networkx (and which will indeed be much faster than networkx when it comes to algorithms like PageRank).  Here is the template for your Graph call, which you'll fill in throughout the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import heapdict\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize with an empty edge dictionary. \"\"\"\n",
    "        self.edges = {}\n",
    "        self.nodes = set()\n",
    "        self.A = False\n",
    "        self.nlist = []\n",
    "    \n",
    "    def add_edges(self, edges_list):\n",
    "        \"\"\" Add a list of edges to the network.\n",
    "        \n",
    "        Args:\n",
    "            edges_list: list of (a,b) tuples, where a->b is an edge to add\n",
    "        \"\"\"       \n",
    "        for left, right in edges_list:\n",
    "            if left in self.edges:\n",
    "                 self.edges[left].update({right:1.0})\n",
    "            else:\n",
    "                self.nodes.add(left)\n",
    "                self.edges[left] = {right:1.0}\n",
    "            \n",
    "            if right not in self.edges:\n",
    "                self.nodes.add(right)                \n",
    "                self.edges[right] ={}\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_neighbors(self, node):\n",
    "        neighbors = set(self.edges[node].keys())\n",
    "        return neighbors\n",
    "    \n",
    "    def shortest_path(self, source):\n",
    "        \"\"\" Compute the single-source shorting path.\n",
    "        \n",
    "        This function uses Djikstra's algorithm to compute the distance from \n",
    "        source to all other nodes in the network.\n",
    "        \n",
    "        Args:\n",
    "            source: node index for the source\n",
    "            \n",
    "        Returns: tuple: dist, path\n",
    "            dist: dictionary of node:distance values for each node in the graph, \n",
    "                  where distance denotes the shortest path distance from source\n",
    "            path: dictionary of node:prev_node values, where prev_node indicates\n",
    "                  the previous node on the path from source to node\n",
    "        \"\"\"\n",
    "        \n",
    "        Q = self.nodes.copy()\n",
    "        dist = {}\n",
    "        path = {}\n",
    "\n",
    "        for vertex in Q:\n",
    "            dist[vertex] = float(\"inf\")\n",
    "            path[vertex] = None\n",
    "\n",
    "        # Distance from source to itself\n",
    "        dist[source] = 0 \n",
    "\n",
    "        hdist = heapdict.heapdict(dist)                       \n",
    "\n",
    "        while Q:\n",
    "            \n",
    "            debugdict = self.__dict__\n",
    "\n",
    "            # get subset of dist dict where key in Q\n",
    "#             distQ = { key:value for key,value in dist.items() if key in Q }\n",
    "\n",
    "            # find key with smallest dist \n",
    "            u = hdist.popitem()[0]\n",
    "            \n",
    "            #remove smallest dist key from set Q\n",
    "            Q.remove(u)\n",
    "            \n",
    "            # find neighbors of u\n",
    "            neighbors = self.get_neighbors(u)\n",
    "            \n",
    "            # filter for neighbors in Q\n",
    "            neighbors = neighbors.intersection(Q)\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                alt = dist[u] + 1\n",
    "                if alt < dist[neighbor]:\n",
    "                    dist[neighbor] = alt \n",
    "                    hdist[neighbor] = alt \n",
    "                    path[neighbor] = u \n",
    "\n",
    "        return dist, path\n",
    "\n",
    "        \n",
    "    def adjacency_matrix(self):\n",
    "        \"\"\" Compute an adjacency matrix form of the graph.  \n",
    "        \n",
    "        Returns: tuple (A, nodes)\n",
    "            A: a sparse matrix in COO form that represents the adjaency matrix\n",
    "               for the graph (i.e., A[j,i] = 1 iff there is an edge i->j)\n",
    "               NOTE: be sure you have this ordering correct!\n",
    "            nodes: a list of nodes indicating the node key corresponding to each\n",
    "                   index of the A matrix\n",
    "        \"\"\"\n",
    "        nodes = list(self.nodes)\n",
    "        nodeIndexDict = {node:i for i,node in enumerate(nodes)}\n",
    "        col  = np.array([nodeIndexDict[src_key] for src_key, valdict in self.edges.iteritems() for trgt_key in valdict.iterkeys()])\n",
    "        row  = np.array([nodeIndexDict[trgt_key] for src_key, valdict in self.edges.iteritems() for trgt_key in valdict.iterkeys()])\n",
    "\n",
    "        data = np.ones(len(row), dtype=np.int)\n",
    "        \n",
    "        A = sp.coo_matrix((data, (row,col)))\n",
    "        \n",
    "        self.A = A\n",
    "        self.nlist = nodes\n",
    "        return (A, nodes)\n",
    "    \n",
    "    def pagerank(self, d=0.85, iters=100):\n",
    "        \"\"\" Compute the PageRank score for each node in the network.\n",
    "        \n",
    "        Compute PageRank scores using the power method.\n",
    "        \n",
    "        Args:\n",
    "            d: 1 - random restart factor\n",
    "            iters: maximum number of iterations of power method\n",
    "            \n",
    "        Returns: dict ranks\n",
    "            ranks: a dictionary of node:importance score, for each node in the\n",
    "                   network (larger score means higher rank)\n",
    "        \n",
    "        \"\"\"\n",
    "        A, nlist = self.adjacency_matrix()\n",
    "        \n",
    "        sumA = np.array(A.sum(axis=0))[0]\n",
    "        \n",
    "        invSumA = 1.0/sumA\n",
    "        \n",
    "        xpd_invSumA = np.array([invSumA[col] for col in A.col])\n",
    "        A.data = A.data*xpd_invSumA*d\n",
    "        \n",
    "        # intialize ranks to uniform dist\n",
    "        rank = np.array([1.0/len(nlist)]*len(nlist))\n",
    "\n",
    "        for i in range(iters):\n",
    "            rank = A.dot(rank) + (1-d)*1.0/len(nlist)\n",
    "        \n",
    "        rank = rank/sum(rank)\n",
    "        ranks = dict(zip(nlist,rank))\n",
    "        return ranks\n",
    "        \n",
    "         \n",
    "# HANDOUT_END  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'B': 1.0, 'D': 1.0}, 'C': {}, 'B': {'C': 1.0}, 'E': {'B': 1.0}, 'D': {'E': 1.0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B', 'D'}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "G = Graph()\n",
    "G.add_edges([(\"A\",\"B\"), (\"B\",\"C\"), (\"A\",\"D\"), (\"D\", \"E\"), (\"E\", \"B\")])\n",
    "print G.edges\n",
    "\n",
    "G.get_neighbors(\"A\")\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'B': 1.0, 'D': 1.0},\n",
       " 'B': {'C': 1.0},\n",
       " 'C': {},\n",
       " 'D': {'E': 1.0},\n",
       " 'E': {'B': 1.0}}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "\n",
    "{\"A\":{\"B\":1.0,\n",
    "    \"D\":1.0}, \n",
    " \"B\":{\"C\":1.0},\n",
    " \"C\":{},\n",
    " \"D\":{\"E\":1.0}, \n",
    " \"E\":{\"B\":1.0}}\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Implement `add_edges()` call\n",
    "\n",
    "To begin, implement the function `add_edges()`.  This will modify the `self.edges` variable to add all edges passed as tuples in `edges_list`.  Note that `self.edges` should be represented as an \"adjacency dictionary\", so that for every node `i` in the graph\n",
    "\n",
    "    self.edges[i]\n",
    "    \n",
    "indicators another dictionary of nodes that `i` is connected to (with the direction pointing from `i` to the other node).  For instance, if `self.edges[i][j]` is a valid entry this means that there is an edge from node `i` to node `j` (the value of this entry doesn't matter, so we could technically use a dictionary of sets, but we use a dictionary of dictionaries to keep things a little bit more uniform and to allow for potential extensions e.g. to weighted graphs).\n",
    "\n",
    "For instance, the following code populates a simple graph like the following:\n",
    "\n",
    "    A -> B -> C\n",
    "    |    ^\n",
    "    v    |\n",
    "    D -> E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'B': 1.0, 'D': 1.0}, 'C': {}, 'B': {'C': 1.0}, 'E': {'B': 1.0}, 'D': {'E': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "G = Graph()\n",
    "G.add_edges([(\"A\",\"B\"), (\"B\",\"C\"), (\"A\",\"D\"), (\"D\", \"E\"), (\"E\", \"B\")])\n",
    "print G.edges\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In addition, make sure your code is able to load the Wikipedia graph from above.  For reference, our routine takes about 22 second to load the full graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Djisktra's algorithm\n",
    "\n",
    "Next, implement Djisktra's single-source shortest path algorithm (with the simple case where the distance along any edge is assumed to be one).  You can refer to the Wikipedia page on Djikstra's algorithm for reference (https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm), but the basic idea of the algorithm is to keep a priority queue of the current known distances to a node.  We continually pop off the smallest element `i` in the queue, and then update all its successor nodes to have a distance of 1 + distance[i].\n",
    "\n",
    "For the priority queue, you should use the heapdict library (https://github.com/DanielStutzbach/heapdict) that is included above (and included with Anaconda).  You can update the priority of a heapdict element and pop off the smallest element using the syntax\n",
    "\n",
    "    d = heapdict({\"a\":5, \"b\":6})\n",
    "    d[\"b\"] = 4\n",
    "    d.popitem() # -> (\"b\", 4)\n",
    "    ...\n",
    "    \n",
    "The function should return a list both of the shortest distances and the previous nodes in the shortest path from the source node to this node.  For instance, executeing the algorithm on our graph above gives the following result.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ({'A': 0, 'C': 2, 'B': 1, 'E': 2, 'D': 1}, {'A': None, 'C': 'B', 'B': 'A', 'E': 'D', 'D': 'A'})\n",
      "B ({'A': inf, 'C': 1, 'B': 0, 'E': inf, 'D': inf}, {'A': None, 'C': 'B', 'B': None, 'E': None, 'D': None})\n",
      "C ({'A': inf, 'C': 0, 'B': inf, 'E': inf, 'D': inf}, {'A': None, 'C': None, 'B': None, 'E': None, 'D': None})\n",
      "D ({'A': inf, 'C': 3, 'B': 2, 'E': 1, 'D': 0}, {'A': None, 'C': 'B', 'B': 'E', 'E': 'D', 'D': None})\n",
      "E ({'A': inf, 'C': 2, 'B': 1, 'E': 0, 'D': inf}, {'A': None, 'C': 'B', 'B': 'E', 'E': None, 'D': None})\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "print \"A\", G.shortest_path(\"A\")\n",
    "print \"B\", G.shortest_path(\"B\")\n",
    "print \"C\", G.shortest_path(\"C\")\n",
    "print \"D\", G.shortest_path(\"D\")\n",
    "print \"E\", G.shortest_path(\"E\")\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates, for instance, that the shortest path from A to C has length 2 and is given by the path `A -> B -> C`.  You should also test your function on the wikipedia graph and verify that it gives the same results as in networkx version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Adjacency matrix representation\n",
    "\n",
    "Implement the adjacency matrix method of the Graph class.  This returns a matrix representing the adjacency of the graph (in scipy COO sparse format), as well as a list of nodes that indicate how the indices in this graph relate to the nodes in the network.\n",
    "\n",
    "IMPORTANT: in order to complete this question in a manner that works on the Wikipedia, you will _need_ to implement this function natively as a sparse matrix (i.e., you cannot construct a dense matrix and then convert that to a sparse matrix, but need to directly use the `sp.coo_matrix()` constructor).  The Wikipedia graph is is 24K x 24K nodes, which (assuming 8 bytes per entry, would take up 4GB of memory.  While it's not impossible to do things this way at this scale (it quickly becomes infeasible for graphs that are even slightly larger), it's a very bad idea, and just allocating this much memory will take too long.\n",
    "\n",
    "For example, in our graph above we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]]\n",
      "['A', 'C', 'B', 'E', 'D']\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "A, nlist = G.adjacency_matrix()\n",
    "print type(A)\n",
    "print A.todense()\n",
    "print nlist\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, make sure your code works on the Wikipedia graph.  In our implementation, it takes 9 seconds to run on this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: PageRank algorithm\n",
    "\n",
    "Finally, implement the PageRank algorithm using the adjacency matrix representation that you built in the previous question.  Reference on the PageRank algorithm is here: https://en.wikipedia.org/wiki/PageRank.  You should specifically use the approach described in the \"Power Method\" section on this page, which we also discussed in class.  This involves forming some initial uniform probability vector $x$, and repeatly multiplying it by the matrices:\n",
    "\\begin{equation}\n",
    "x := (d P + ((1-d)/n) E)x\n",
    "\\end{equation}\n",
    "where $P$ is a transition matrix (the $A$ matrix normalized so that all columns have sum 1), $E$ is the matrix of all ones, and $d$ is the damping factor.\n",
    "\n",
    "NOTES: Recall that from the definition of PageRank, when we reach a \"sink\" node (a node with no outgoing edges), we randomly hope to any other node in the network, so that columns of $P$ that have no outgoing edges are set to the uniform distribution.  To be efficient, you'll also want to avoid explicitly forming the $E$ matrix, and should instead use the fact that $E = 11^T$ where $1$ denotes a vector of all ones.  Use the fact that we can reorder matrix multiplication if associative (i.e., the fact the $A(BC)$ = $(AB)C$) to make this operation as fast as possible.\n",
    "\n",
    "Your function should return a dictionary of nodes and their corresponding page rank.  For example, in our graph above, we have the following reults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\ipykernel\\__main__.py:147: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 0.085108623870681688,\n",
       " 'B': 0.28124676686965955,\n",
       " 'C': 0.32416837570989232,\n",
       " 'D': 0.12127978901572142,\n",
       " 'E': 0.18819644453404491}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "G.pagerank()\n",
    "\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "W = Graph()\n",
    "W.add_edges(wiki_edgelst)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\dato-env\\lib\\site-packages\\ipykernel\\__main__.py:147: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "wikirank_raw = W.pagerank()\n",
    "wikirank = {nodedict[int(k)]:v for k, v in wikirank_raw.items()}\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('United_States', 0.0027518870536402955),\n",
       " ('2007', 0.0024433905407855007),\n",
       " ('2008', 0.0021734251483718826),\n",
       " ('Wikimedia_Commons', 0.0017260420017522586),\n",
       " ('United_Kingdom', 0.0015999885109100329),\n",
       " ('2006', 0.0015484491173122946),\n",
       " ('France', 0.0014495899414189285),\n",
       " ('Wiktionary', 0.0012675347642074604),\n",
       " ('Canada', 0.0010989619526347431),\n",
       " ('World_War_II', 0.0010491307968022643),\n",
       " ('2005', 0.001046330246169557),\n",
       " ('List_of_African_films', 0.0010071387023193896),\n",
       " ('Germany', 0.00095626219282239134),\n",
       " ('Europe', 0.00093769002565240655),\n",
       " ('English_language', 0.00090814436025853952)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "result = sorted(wikirank.items(), key=lambda x: x[1], reverse=True)\n",
    "print sum(r[1] for r in result)\n",
    "result[0:15]\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is intuitive, nodes B and C have higher page rank, as they are pointed to by more of the other nodes.\n",
    "\n",
    "Make sure your implementation works on the full Wikipedia graph (in our implementation, it takes 11 seconds to run, most of which is taken up by generating the adjacency matrix).  The top PageRank entires we get from our implementation are (drumroll...)\n",
    "\n",
    "    United_States 0.00275188705264\n",
    "    2007 0.00244339053989\n",
    "    2008 0.00217342514773\n",
    "    Wikimedia_Commons 0.00172604200122\n",
    "    United_Kingdom 0.00159998851013\n",
    "    2006 0.00154844911674\n",
    "    France 0.00144958994072\n",
    "    Wiktionary 0.00126753476354\n",
    "    Canada 0.00109896195215\n",
    "    World_War_II 0.00104913079624\n",
    "    2005 0.00104633024568\n",
    "    List_of_African_films 0.00100713870383\n",
    "    Germany 0.000956262192248\n",
    "    Europe 0.000937690025073\n",
    "    English_language 0.000908144359626\n",
    "    Geographic_coordinate_system 0.000891711151403\n",
    "    Latin 0.000888662228804\n",
    "    Australia 0.000879854710005\n",
    "    India 0.000787625093175\n",
    "    Japan 0.000784815389935\n",
    "\n",
    "countries and years! (and wikipedia links and a couple other rather interesting items).  But overall a seemingly reasonable list of pages we may expect to be important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
